{"format": "torch", "nodes": [{"name": "shared", "id": 140119601167056, "class_name": "Embedding(250100, 512)", "parameters": [["weight", [250100, 512]]], "output_shape": [[1, 26, 512]], "num_parameters": [128051200]}, {"name": "encoder", "id": 140119831008272, "class_name": "T5Stack(\n  (embed_tokens): Embedding(250100, 512)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n            (relative_attention_bias): Embedding(32, 6)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (6): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (7): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)", "parameters": [["embed_tokens.weight", [250100, 512]], ["block.0.layer.0.SelfAttention.q.weight", [384, 512]], ["block.0.layer.0.SelfAttention.k.weight", [384, 512]], ["block.0.layer.0.SelfAttention.v.weight", [384, 512]], ["block.0.layer.0.SelfAttention.o.weight", [512, 384]], ["block.0.layer.0.SelfAttention.relative_attention_bias.weight", [32, 6]], ["block.0.layer.0.layer_norm.weight", [512]], ["block.0.layer.1.DenseReluDense.wi_0.weight", [1024, 512]], ["block.0.layer.1.DenseReluDense.wi_1.weight", [1024, 512]], ["block.0.layer.1.DenseReluDense.wo.weight", [512, 1024]], ["block.0.layer.1.layer_norm.weight", [512]], ["block.1.layer.0.SelfAttention.q.weight", [384, 512]], ["block.1.layer.0.SelfAttention.k.weight", [384, 512]], ["block.1.layer.0.SelfAttention.v.weight", [384, 512]], ["block.1.layer.0.SelfAttention.o.weight", [512, 384]], ["block.1.layer.0.layer_norm.weight", [512]], ["block.1.layer.1.DenseReluDense.wi_0.weight", [1024, 512]], ["block.1.layer.1.DenseReluDense.wi_1.weight", [1024, 512]], ["block.1.layer.1.DenseReluDense.wo.weight", [512, 1024]], ["block.1.layer.1.layer_norm.weight", [512]], ["block.2.layer.0.SelfAttention.q.weight", [384, 512]], ["block.2.layer.0.SelfAttention.k.weight", [384, 512]], ["block.2.layer.0.SelfAttention.v.weight", [384, 512]], ["block.2.layer.0.SelfAttention.o.weight", [512, 384]], ["block.2.layer.0.layer_norm.weight", [512]], ["block.2.layer.1.DenseReluDense.wi_0.weight", [1024, 512]], ["block.2.layer.1.DenseReluDense.wi_1.weight", [1024, 512]], ["block.2.layer.1.DenseReluDense.wo.weight", [512, 1024]], ["block.2.layer.1.layer_norm.weight", [512]], ["block.3.layer.0.SelfAttention.q.weight", [384, 512]], ["block.3.layer.0.SelfAttention.k.weight", [384, 512]], ["block.3.layer.0.SelfAttention.v.weight", [384, 512]], ["block.3.layer.0.SelfAttention.o.weight", [512, 384]], ["block.3.layer.0.layer_norm.weight", [512]], ["block.3.layer.1.DenseReluDense.wi_0.weight", [1024, 512]], ["block.3.layer.1.DenseReluDense.wi_1.weight", [1024, 512]], ["block.3.layer.1.DenseReluDense.wo.weight", [512, 1024]], ["block.3.layer.1.layer_norm.weight", [512]], ["block.4.layer.0.SelfAttention.q.weight", [384, 512]], ["block.4.layer.0.SelfAttention.k.weight", [384, 512]], ["block.4.layer.0.SelfAttention.v.weight", [384, 512]], ["block.4.layer.0.SelfAttention.o.weight", [512, 384]], ["block.4.layer.0.layer_norm.weight", [512]], ["block.4.layer.1.DenseReluDense.wi_0.weight", [1024, 512]], ["block.4.layer.1.DenseReluDense.wi_1.weight", [1024, 512]], ["block.4.layer.1.DenseReluDense.wo.weight", [512, 1024]], ["block.4.layer.1.layer_norm.weight", [512]], ["block.5.layer.0.SelfAttention.q.weight", [384, 512]], ["block.5.layer.0.SelfAttention.k.weight", [384, 512]], ["block.5.layer.0.SelfAttention.v.weight", [384, 512]], ["block.5.layer.0.SelfAttention.o.weight", [512, 384]], ["block.5.layer.0.layer_norm.weight", [512]], ["block.5.layer.1.DenseReluDense.wi_0.weight", [1024, 512]], ["block.5.layer.1.DenseReluDense.wi_1.weight", [1024, 512]], ["block.5.layer.1.DenseReluDense.wo.weight", [512, 1024]], ["block.5.layer.1.layer_norm.weight", [512]], ["block.6.layer.0.SelfAttention.q.weight", [384, 512]], ["block.6.layer.0.SelfAttention.k.weight", [384, 512]], ["block.6.layer.0.SelfAttention.v.weight", [384, 512]], ["block.6.layer.0.SelfAttention.o.weight", [512, 384]], ["block.6.layer.0.layer_norm.weight", [512]], ["block.6.layer.1.DenseReluDense.wi_0.weight", [1024, 512]], ["block.6.layer.1.DenseReluDense.wi_1.weight", [1024, 512]], ["block.6.layer.1.DenseReluDense.wo.weight", [512, 1024]], ["block.6.layer.1.layer_norm.weight", [512]], ["block.7.layer.0.SelfAttention.q.weight", [384, 512]], ["block.7.layer.0.SelfAttention.k.weight", [384, 512]], ["block.7.layer.0.SelfAttention.v.weight", [384, 512]], ["block.7.layer.0.SelfAttention.o.weight", [512, 384]], ["block.7.layer.0.layer_norm.weight", [512]], ["block.7.layer.1.DenseReluDense.wi_0.weight", [1024, 512]], ["block.7.layer.1.DenseReluDense.wi_1.weight", [1024, 512]], ["block.7.layer.1.DenseReluDense.wo.weight", [512, 1024]], ["block.7.layer.1.layer_norm.weight", [512]], ["final_layer_norm.weight", [512]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0]]], "num_parameters": [128051200, 196608, 196608, 196608, 196608, 192, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 512]}, {"name": "decoder", "id": 140119831010512, "class_name": "T5Stack(\n  (embed_tokens): Embedding(250100, 512)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n            (relative_attention_bias): Embedding(32, 6)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (6): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (7): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedGeluDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)", "parameters": [["embed_tokens.weight", [250100, 512]], ["block.0.layer.0.SelfAttention.q.weight", [384, 512]], ["block.0.layer.0.SelfAttention.k.weight", [384, 512]], ["block.0.layer.0.SelfAttention.v.weight", [384, 512]], ["block.0.layer.0.SelfAttention.o.weight", [512, 384]], ["block.0.layer.0.SelfAttention.relative_attention_bias.weight", [32, 6]], ["block.0.layer.0.layer_norm.weight", [512]], ["block.0.layer.1.EncDecAttention.q.weight", [384, 512]], ["block.0.layer.1.EncDecAttention.k.weight", [384, 512]], ["block.0.layer.1.EncDecAttention.v.weight", [384, 512]], ["block.0.layer.1.EncDecAttention.o.weight", [512, 384]], ["block.0.layer.1.layer_norm.weight", [512]], ["block.0.layer.2.DenseReluDense.wi_0.weight", [1024, 512]], ["block.0.layer.2.DenseReluDense.wi_1.weight", [1024, 512]], ["block.0.layer.2.DenseReluDense.wo.weight", [512, 1024]], ["block.0.layer.2.layer_norm.weight", [512]], ["block.1.layer.0.SelfAttention.q.weight", [384, 512]], ["block.1.layer.0.SelfAttention.k.weight", [384, 512]], ["block.1.layer.0.SelfAttention.v.weight", [384, 512]], ["block.1.layer.0.SelfAttention.o.weight", [512, 384]], ["block.1.layer.0.layer_norm.weight", [512]], ["block.1.layer.1.EncDecAttention.q.weight", [384, 512]], ["block.1.layer.1.EncDecAttention.k.weight", [384, 512]], ["block.1.layer.1.EncDecAttention.v.weight", [384, 512]], ["block.1.layer.1.EncDecAttention.o.weight", [512, 384]], ["block.1.layer.1.layer_norm.weight", [512]], ["block.1.layer.2.DenseReluDense.wi_0.weight", [1024, 512]], ["block.1.layer.2.DenseReluDense.wi_1.weight", [1024, 512]], ["block.1.layer.2.DenseReluDense.wo.weight", [512, 1024]], ["block.1.layer.2.layer_norm.weight", [512]], ["block.2.layer.0.SelfAttention.q.weight", [384, 512]], ["block.2.layer.0.SelfAttention.k.weight", [384, 512]], ["block.2.layer.0.SelfAttention.v.weight", [384, 512]], ["block.2.layer.0.SelfAttention.o.weight", [512, 384]], ["block.2.layer.0.layer_norm.weight", [512]], ["block.2.layer.1.EncDecAttention.q.weight", [384, 512]], ["block.2.layer.1.EncDecAttention.k.weight", [384, 512]], ["block.2.layer.1.EncDecAttention.v.weight", [384, 512]], ["block.2.layer.1.EncDecAttention.o.weight", [512, 384]], ["block.2.layer.1.layer_norm.weight", [512]], ["block.2.layer.2.DenseReluDense.wi_0.weight", [1024, 512]], ["block.2.layer.2.DenseReluDense.wi_1.weight", [1024, 512]], ["block.2.layer.2.DenseReluDense.wo.weight", [512, 1024]], ["block.2.layer.2.layer_norm.weight", [512]], ["block.3.layer.0.SelfAttention.q.weight", [384, 512]], ["block.3.layer.0.SelfAttention.k.weight", [384, 512]], ["block.3.layer.0.SelfAttention.v.weight", [384, 512]], ["block.3.layer.0.SelfAttention.o.weight", [512, 384]], ["block.3.layer.0.layer_norm.weight", [512]], ["block.3.layer.1.EncDecAttention.q.weight", [384, 512]], ["block.3.layer.1.EncDecAttention.k.weight", [384, 512]], ["block.3.layer.1.EncDecAttention.v.weight", [384, 512]], ["block.3.layer.1.EncDecAttention.o.weight", [512, 384]], ["block.3.layer.1.layer_norm.weight", [512]], ["block.3.layer.2.DenseReluDense.wi_0.weight", [1024, 512]], ["block.3.layer.2.DenseReluDense.wi_1.weight", [1024, 512]], ["block.3.layer.2.DenseReluDense.wo.weight", [512, 1024]], ["block.3.layer.2.layer_norm.weight", [512]], ["block.4.layer.0.SelfAttention.q.weight", [384, 512]], ["block.4.layer.0.SelfAttention.k.weight", [384, 512]], ["block.4.layer.0.SelfAttention.v.weight", [384, 512]], ["block.4.layer.0.SelfAttention.o.weight", [512, 384]], ["block.4.layer.0.layer_norm.weight", [512]], ["block.4.layer.1.EncDecAttention.q.weight", [384, 512]], ["block.4.layer.1.EncDecAttention.k.weight", [384, 512]], ["block.4.layer.1.EncDecAttention.v.weight", [384, 512]], ["block.4.layer.1.EncDecAttention.o.weight", [512, 384]], ["block.4.layer.1.layer_norm.weight", [512]], ["block.4.layer.2.DenseReluDense.wi_0.weight", [1024, 512]], ["block.4.layer.2.DenseReluDense.wi_1.weight", [1024, 512]], ["block.4.layer.2.DenseReluDense.wo.weight", [512, 1024]], ["block.4.layer.2.layer_norm.weight", [512]], ["block.5.layer.0.SelfAttention.q.weight", [384, 512]], ["block.5.layer.0.SelfAttention.k.weight", [384, 512]], ["block.5.layer.0.SelfAttention.v.weight", [384, 512]], ["block.5.layer.0.SelfAttention.o.weight", [512, 384]], ["block.5.layer.0.layer_norm.weight", [512]], ["block.5.layer.1.EncDecAttention.q.weight", [384, 512]], ["block.5.layer.1.EncDecAttention.k.weight", [384, 512]], ["block.5.layer.1.EncDecAttention.v.weight", [384, 512]], ["block.5.layer.1.EncDecAttention.o.weight", [512, 384]], ["block.5.layer.1.layer_norm.weight", [512]], ["block.5.layer.2.DenseReluDense.wi_0.weight", [1024, 512]], ["block.5.layer.2.DenseReluDense.wi_1.weight", [1024, 512]], ["block.5.layer.2.DenseReluDense.wo.weight", [512, 1024]], ["block.5.layer.2.layer_norm.weight", [512]], ["block.6.layer.0.SelfAttention.q.weight", [384, 512]], ["block.6.layer.0.SelfAttention.k.weight", [384, 512]], ["block.6.layer.0.SelfAttention.v.weight", [384, 512]], ["block.6.layer.0.SelfAttention.o.weight", [512, 384]], ["block.6.layer.0.layer_norm.weight", [512]], ["block.6.layer.1.EncDecAttention.q.weight", [384, 512]], ["block.6.layer.1.EncDecAttention.k.weight", [384, 512]], ["block.6.layer.1.EncDecAttention.v.weight", [384, 512]], ["block.6.layer.1.EncDecAttention.o.weight", [512, 384]], ["block.6.layer.1.layer_norm.weight", [512]], ["block.6.layer.2.DenseReluDense.wi_0.weight", [1024, 512]], ["block.6.layer.2.DenseReluDense.wi_1.weight", [1024, 512]], ["block.6.layer.2.DenseReluDense.wo.weight", [512, 1024]], ["block.6.layer.2.layer_norm.weight", [512]], ["block.7.layer.0.SelfAttention.q.weight", [384, 512]], ["block.7.layer.0.SelfAttention.k.weight", [384, 512]], ["block.7.layer.0.SelfAttention.v.weight", [384, 512]], ["block.7.layer.0.SelfAttention.o.weight", [512, 384]], ["block.7.layer.0.layer_norm.weight", [512]], ["block.7.layer.1.EncDecAttention.q.weight", [384, 512]], ["block.7.layer.1.EncDecAttention.k.weight", [384, 512]], ["block.7.layer.1.EncDecAttention.v.weight", [384, 512]], ["block.7.layer.1.EncDecAttention.o.weight", [512, 384]], ["block.7.layer.1.layer_norm.weight", [512]], ["block.7.layer.2.DenseReluDense.wi_0.weight", [1024, 512]], ["block.7.layer.2.DenseReluDense.wi_1.weight", [1024, 512]], ["block.7.layer.2.DenseReluDense.wo.weight", [512, 1024]], ["block.7.layer.2.layer_norm.weight", [512]], ["final_layer_norm.weight", [512]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0], [[0], 0, 0, 0, 0, [0], 0, [0], 0, [0], 0, 0, [0], 0, 0]]], "num_parameters": [128051200, 196608, 196608, 196608, 196608, 192, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 196608, 196608, 196608, 196608, 512, 196608, 196608, 196608, 196608, 512, 524288, 524288, 524288, 512, 512]}, {"name": "lm_head", "id": 140119601165328, "class_name": "Linear(in_features=512, out_features=250100, bias=False)", "parameters": [["weight", [250100, 512]]], "output_shape": [[1, 20, 250100]], "num_parameters": [128051200]}], "edges": []}