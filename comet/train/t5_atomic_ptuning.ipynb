{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "title,-all",
      "formats": "ipynb,py",
      "main_language": "python"
    },
    "colab": {
      "name": "t5_atomic_ptuning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j__TjBDRs9dT"
      },
      "source": [
        "!git clone https://github.com/puraminy/mt5-comet.git\n",
        "%cd /content/mt5-comet\n",
        "!pip install -e .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "title": "load libraries",
        "id": "--aXysz2s8Xx"
      },
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast, AdamW, AddedToken,\n",
        "    MT5ForConditionalGeneration, MT5TokenizerFast,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from comet.transformers_ptuning import PTuningWrapper\n",
        "import torch\n",
        "import csv \n",
        "import re\n",
        "import json\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os,time\n",
        "import argparse\n",
        "from tqdm.auto import tqdm\n",
        "from comet.transformers_ptuning.ptuning_wrapper import LSTMEmbeddingPromptEncoder, EmbeddingPromptEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "title": "argparse",
        "id": "CERgNGgPs8X3"
      },
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import click\n",
        "from tqdm import tqdm\n",
        "@click.command()\n",
        "@click.argument(\"model_id\", type=str)\n",
        "@click.argument(\"exp_id\", type=str)\n",
        "@click.option(\n",
        "    \"--path\",\n",
        "    envvar=\"PWD\",\n",
        "    #    multiple=True,\n",
        "    type=click.Path(),\n",
        "    help=\"The current path (it is set by system)\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--iterations\",\n",
        "    \"-i\",\n",
        "    default=5000,\n",
        "    type=int,\n",
        "    help=\"\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--cycle\",\n",
        "    \"-c\",\n",
        "    default=1000,\n",
        "    type=int,\n",
        "    help=\"\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--frozen\",\n",
        "    \"-f\",\n",
        "    is_flag=True,\n",
        "    help=\"keep model frozen or not\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--sup\",\n",
        "    \"-s\",\n",
        "    is_flag=True,\n",
        "    help=\"supervised flag\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--qtemp\",\n",
        "    default=\"{rel} {event} {ph}\",\n",
        "    type=str,\n",
        "    help=\"template for query\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--anstemp\",\n",
        "    default=\"{ph} {response} {end}\",\n",
        "    type=str,\n",
        "    help=\"tempate for response\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--beams\",\n",
        "    \"-nb\",\n",
        "    default=5,\n",
        "    type=int,\n",
        "    help=\"number of beams\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--ret_seq\",\n",
        "    \"-r\",\n",
        "    default=5,\n",
        "    type=int,\n",
        "    help=\"number of return sequences\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--num_generations\",\n",
        "    \"-g\",\n",
        "    default=0,\n",
        "    type=int,\n",
        "    help=\"\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--is_flax\",\n",
        "    \"-if\",\n",
        "    is_flag=True,\n",
        "    help=\"If the model is flaxed it converst it to pytorch compbatible model and save it\"\n",
        ")\n",
        "@click.option(\n",
        "    \"--en\",\n",
        "    \"-en\",\n",
        "    is_flag=True,\n",
        "    help=\"The languge of head and tails\"\n",
        ")\n",
        "def main(model_id, exp_id, path, iterations, cycle, frozen, sup, qtemp, anstemp, beams, ret_seq, num_generations, is_flax, en):\n",
        "    local_rank = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "tHFj8ncUs8X5"
      },
      "source": [
        "    cfg = {}\n",
        "    old_vars = set()\n",
        "    old_vars.update(k for k in locals() if not k.startswith('_'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "title": "some hyper-parameters",
        "id": "UmH6uXyls8X6"
      },
      "source": [
        "    if not model_id == \"path\":\n",
        "        underlying_model_name = f\"/drive2/pretrained/mt5/hf/{model_id}/\"\n",
        "    else:\n",
        "        underlying_model_name = path\n",
        "    if not Path(underlying_model_name).exists():\n",
        "        underlying_model_name = model_id\n",
        "\n",
        "    #underlying_model_name = \"logs/mt5-small/prompt_length_3/last\"\n",
        "    if sup:\n",
        "        learning_rate = 0.0001 #6.25e-05\n",
        "    else:\n",
        "        learning_rate = 0.01 #6.25e-05\n",
        "    #iiiii\n",
        "    frozen_str = \"_frozen\" if frozen else \"_UNfrozen\"\n",
        "    sup_str = \"_SUP\" if sup else \"_UNsup\"\n",
        "    iter_str = \"_\" + str(iterations).replace(\"000\",\"k\")\n",
        "    prompt_length = 5\n",
        "    split_col={\"train\":{}, \"validation\":{}}\n",
        "    split_col[\"train\"][\"input_text\"]=\"input_text\" if en else \"input_text_fa\"\n",
        "    split_col[\"train\"][\"target_text\"]=\"target_text\" if en else \"target_text_fa\"\n",
        "    split_col[\"validation\"][\"input_text\"]=\"input_text\" if en else \"input_text_fa\"\n",
        "    split_col[\"validation\"][\"target_text\"]=\"target_text\" if en else \"target_text_fa\"\n",
        "    warm_up_steps = 0.002*iterations\n",
        "    weight_decay = 0.01\n",
        "    batch_size = 16\n",
        "    #% creating save folder\n",
        "    log_dir = '/drive2/pouramini/plogs/'\n",
        "    train_folder=split_col[\"train\"][\"input_text\"] + \"--\" + split_col[\"train\"][\"target_text\"]\n",
        "    val_folder=split_col[\"validation\"][\"input_text\"] + \"--\" + split_col[\"validation\"][\"target_text\"]\n",
        "    model_name = f\"plength_{prompt_length}_lr_{learning_rate}{frozen_str}{sup_str}{iter_str}_{exp_id}\"\n",
        "    model_path = os.path.join(model_id,train_folder, val_folder, model_name)\n",
        "    if model_id != \"path\":\n",
        "        save_path= os.path.join(log_dir, model_path)\n",
        "        ans = \"\"\n",
        "        while Path(save_path).exists() and ans != \"y\":\n",
        "            ans = input(f\"The {save_path} already exists, do you want to overwrite it? (y/n/other(suffix):\")\n",
        "            if ans == \"n\":\n",
        "                return\n",
        "            elif ans != \"y\":\n",
        "                save_path += \"_\" + ans\n",
        "    else:\n",
        "        save_path = path\n",
        "    #Cut down memory usage by accumulating tiny steps for multiple backwards;\n",
        "    #Should be divided exactly by batch_size\n",
        "    accumulation_tiny_steps = 1 \n",
        "    shuffle = True\n",
        "    shuffle_evaluation=False\n",
        "    validation_size = 1000\n",
        "    validation_num_generation = 10\n",
        "    generation_params = {\n",
        "        \"max_length\":80,\n",
        "        \"early_stopping\":True,\n",
        "        \"num_beams\":beams,\n",
        "        \"num_return_sequences\":ret_seq,\n",
        "    }\n",
        "    ddp = local_rank is not None\n",
        "    device = 'cuda'\n",
        "    #dataset_path = \"../../data/v4_atomic_all_agg.csv\"\n",
        "    atomic_relation_prompt_lengths = {\n",
        "        \"xIntent\":prompt_length,\n",
        "        \"oEffect\":prompt_length,\n",
        "        \"oReact\":prompt_length,\n",
        "        \"oWant\":prompt_length,\n",
        "        \"xAttr\":prompt_length,\n",
        "        \"xEffect\":prompt_length,\n",
        "        \"xNeed\":prompt_length,\n",
        "        \"xReact\":prompt_length,\n",
        "        \"xWant\":prompt_length\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "6w8bDDXbs8X7"
      },
      "source": [
        "    atomic_relation_prompt_lengths = {\n",
        "        \"xIntent\":prompt_length,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "Vyy6ekvSs8X8"
      },
      "source": [
        "    new_vars = set(k for k in locals() if not k.startswith('_'))\n",
        "    cfg_vars = new_vars-old_vars\n",
        "    cfg = {k:v for k,v in locals().items() if k in cfg_vars }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "title": "load atomic data",
        "id": "bMZgeBLds8X9"
      },
      "source": [
        "\n",
        "    data_df = {}\n",
        "    #data_df[\"train\"] = pd.read_table(\"/home/pouramini/atomic/xIntent_en_train_no_dups.tsv\")\n",
        "    #data_df[\"validation\"] = pd.read_table(\"/home/pouramini/atomic/xIntent_en_fa_validation_no_dups.tsv\")\n",
        "\n",
        "    data_df[\"train\"] = pd.read_table(\"../../data/xIntent_en_fa_train_no_dups.tsv\")\n",
        "    data_df[\"validation\"] = pd.read_table(\"../../data/xIntent_en_fa_validation_no_dups.tsv\")\n",
        "\n",
        "    def my_load_dataset(split_data, split, target_text, input_text):\n",
        "        data_split = {}\n",
        "        split_data[target_text] = split_data[target_text].astype(str)\n",
        "        for index, d in split_data.iterrows():\n",
        "            rel = d[\"prefix\"]\n",
        "            if len(d[target_text])>0: \n",
        "                event = d[input_text]\n",
        "                if event not in data_split:\n",
        "                    data_split[event] = {\"event\":event, 'split':split}\n",
        "                if not rel in data_split[event]:\n",
        "                    data_split[event][rel] = []\n",
        "                data_split[event][rel].append(d[target_text])\n",
        "                #didn't convert ___ to <blank>\n",
        "                #didn't normalize to lowercase\n",
        "        return list(data_split.values())\n",
        "\n",
        "    def load_atomic_dataset(path):\n",
        "        data={}\n",
        "        with open(path) as source_file:\n",
        "            source_reader = csv.reader(source_file)\n",
        "            # Read first line to get column name\n",
        "            source_line = next(source_reader)\n",
        "            event_colname = source_line[0]\n",
        "            categories_colname = source_line[1:10]\n",
        "            prefix_colname = source_line[10]\n",
        "            split_colname = source_line[11]\n",
        "            for source_line in source_reader:\n",
        "                # get every column\n",
        "                event = source_line[0]\n",
        "                annotationss = [\n",
        "                    json.loads(raw_anns) for raw_anns in source_line[1:10]]\n",
        "                event_prefix = source_line[10]\n",
        "                event_split = source_line[11]\n",
        "                if event_split not in data:\n",
        "                    data[event_split] = []\n",
        "                d = {\"event\":event}\n",
        "                d.update({category:annotations for \n",
        "                    category,annotations in zip(categories_colname,annotationss)})\n",
        "                data[event_split].append(d)\n",
        "        return data\n",
        "\n",
        "    # atomic_dataset = load_atomic_dataset(dataset_path)\n",
        "    #atomic_dataset = load_dataset(\"atomic\")\n",
        "    atomic_dataset = {}\n",
        "    for split, split_data in data_df.items():\n",
        "        print(\"split:\", split)\n",
        "        atomic_dataset[split] = my_load_dataset(split_data, split, split_col[split][\"target_text\"], split_col[split][\"input_text\"])\n",
        "\n",
        "    placeholder_token = \"<extra_id_0>\"\n",
        "    atomic_relation_mappings = {\n",
        "        \"oEffect\":\"<oEffect>\",\n",
        "        \"oReact\":\"<oReact>\",\n",
        "        \"oWant\":\"<oWant>\",\n",
        "        \"xAttr\":\"<xAttr>\",\n",
        "        \"xEffect\":\"<xEffect>\",\n",
        "        \"xIntent\":\"<xIntent>\",\n",
        "        \"xNeed\":\"<xNeed>\",\n",
        "        \"xReact\":\"<xReact>\",\n",
        "        \"xWant\":\"<xWant>\"\n",
        "    }\n",
        "    gen_token = \"<gen>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "title": "dpp initialize",
        "id": "Xu5Rw7fgs8X9"
      },
      "source": [
        "    is_main_process = (not ddp or local_rank == 0) \n",
        "    if ddp:\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        print(\"launch process\",local_rank)\n",
        "        world_size = torch.distributed.get_world_size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "title": "tokenizer & model",
        "id": "DHEVDLU3s8X_"
      },
      "source": [
        "    if \"mt5\" in model_id:\n",
        "        tokenizer = MT5TokenizerFast.from_pretrained(underlying_model_name)\n",
        "        model = MT5ForConditionalGeneration.from_pretrained(underlying_model_name)\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(underlying_model_name)\n",
        "        if is_flax:\n",
        "            model = T5ForConditionalGeneration.from_pretrained(underlying_model_name, from_flax=True)\n",
        "            print(\"converting to \", underlying_model_name + \"X\")\n",
        "\n",
        "            model.save_pretrained(underlying_model_name + \"X\")\n",
        "            tokenizer.save_pretrained(underlying_model_name + \"X\")\n",
        "            return\n",
        "        else:\n",
        "            model = T5ForConditionalGeneration.from_pretrained(underlying_model_name)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = not frozen\n",
        "    allowed_out_token_length = len(tokenizer)\n",
        "    def clip_logits(logits):\n",
        "        return logits[:,:,:allowed_out_token_length]\n",
        "    #可以在这里就加钩子，因为模型不会替换模块，只会更新权重\n",
        "    clip_logits_hook = model.get_output_embeddings().register_forward_hook(\n",
        "        lambda m,i,o:clip_logits(o)\n",
        "    )\n",
        "    # add new tokens\n",
        "    #added_tokens = [ \n",
        "    #     AddedToken(token,lstrip=True,\n",
        "    #         rstrip=False)\n",
        "    #     for token in \n",
        "    #         list(atomic_relation_mappings.values())\n",
        "    #]\n",
        "    #tokenizer.add_special_tokens({\"additional_special_tokens\":added_tokens})\n",
        "    #model.resize_token_embeddings(len(tokenizer))\n",
        "    embedding_dim = model.config.hidden_size\n",
        "    print(\"embedding dim:\", embedding_dim)\n",
        "    wrapped_models = {}\n",
        "    atomic_relation_mappings = {}\n",
        "    def get_prompt_token_fn(id_offset,length):\n",
        "        return lambda x: (x>=id_offset)&(x<id_offset+length)\n",
        "    for rel in atomic_relation_prompt_lengths:\n",
        "        id_offset = len(tokenizer)\n",
        "        print(\"id_offset:\", id_offset)\n",
        "        length = atomic_relation_prompt_lengths[rel]\n",
        "        atomic_relation_mappings[rel] = \" \".join(f\"<{rel}_{i}>\" for i in range(length))\n",
        "        prompt_encoder = LSTMEmbeddingPromptEncoder(length,embedding_dim,id_offset)\n",
        "        added_tokens = [ \n",
        "            AddedToken(f\"<{rel}_{i}>\",lstrip=True,\n",
        "                rstrip=False)\n",
        "            for i in \n",
        "                range(length)\n",
        "        ]\n",
        "        tokenizer.add_special_tokens({\"additional_special_tokens\":added_tokens})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "        wrapped_models[rel] = PTuningWrapper(model,prompt_encoder,prompt_token_fn=get_prompt_token_fn(id_offset,length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "title": "Aggregate instances of queries and corresponding responses",
        "id": "_WQCaloTs8YA"
      },
      "source": [
        "    # (str)split_name -> (dict) query -> (list) response \n",
        "    print(\"building query responses\")\n",
        "    atomic_query_responses = {}\n",
        "    # ttt\n",
        "    for split_name,split_data in atomic_dataset.items():\n",
        "        atomic_query_responses[split_name] = {}\n",
        "        for d in split_data:\n",
        "            for rel in atomic_relation_mappings:\n",
        "                if rel not in atomic_query_responses[split_name]:\n",
        "                    atomic_query_responses[split_name][rel] = {}\n",
        "                if len(d[rel])>0: \n",
        "                    rel_tokens = atomic_relation_mappings[rel]\n",
        "                    #query = f\"{rel_tokens} {d['event']}\" #Change this line to modify the encoder input\n",
        "                    query = qtemp.format(event=d['event'], rel=rel_tokens, ph=placeholder_token) #Change this line to modify the encoder input\n",
        "                    # query = f\"{d['event']} {rel_tokens}\" #Change this line to modify the encoder input\n",
        "                    if query not in atomic_query_responses[split_name][rel]:\n",
        "                        atomic_query_responses[split_name][rel][query] = []\n",
        "                    for response in d[rel]:\n",
        "                        answer = anstemp.format(response=response, rel=rel_tokens, ph=placeholder_token, end=\"<extra_id_1>\")\n",
        "                        atomic_query_responses[split_name][rel][query].append((answer,response))\n",
        "                            #Change this line to modify the decoder input\n",
        "                    #didn't convert ___ to <blank>\n",
        "                    #didn't normalize to lowercase\n",
        "\n",
        "    #flatten\n",
        "    print(\"building flattened pairs\")\n",
        "    atomic_flattened = {}\n",
        "    for split_name,rel_queries_responses in atomic_query_responses.items():\n",
        "        atomic_flattened[split_name] = {}\n",
        "        for rel,queries_responses in rel_queries_responses.items():\n",
        "            atomic_flattened[split_name][rel] = []\n",
        "            for query,responses in queries_responses.items():\n",
        "                for response,_ in responses:\n",
        "                    atomic_flattened[split_name][rel].append((query,response))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "title": "Prepare training data",
        "id": "f0PJ3shZs8YB"
      },
      "source": [
        "\n",
        "    def collate_fn_for_flattened(batch):\n",
        "        queries,responses = zip(*batch)\n",
        "        new_batch = tokenizer(list(queries),return_tensors='pt',padding='longest')\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            outputs = tokenizer(list(responses),return_tensors='pt',padding='longest')\n",
        "            labels = outputs['input_ids']\n",
        "            labels[labels==tokenizer.pad_token_id] = -100\n",
        "            new_batch['labels']=labels\n",
        "        return new_batch\n",
        "\n",
        "    # def collate_fn_for_generation(batch):\n",
        "    #     queries,references = zip(*batch)\n",
        "    #     new_batch = tokenizer(queries,return_tensors='pt',padding='longest')\n",
        "    #     return new_batch,references"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "title": "build dataloader",
        "id": "5ATGCDfOs8YB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "title": "dataloader and  parallel",
        "id": "wHuLpDJcs8YB"
      },
      "source": [
        "    node_batch_size = batch_size//accumulation_tiny_steps\n",
        "    train_sampler = {}\n",
        "    train_dataloader = {}\n",
        "    dev_dataloader = {}\n",
        "    for rel in atomic_relation_mappings:\n",
        "        train_sampler[rel] = None\n",
        "        if shuffle:\n",
        "            train_sampler[rel] = torch.utils.data.RandomSampler(atomic_flattened['train'][rel])\n",
        "        if ddp:\n",
        "            assert node_batch_size%world_size == 0\n",
        "            node_batch_size = node_batch_size//world_size\n",
        "            train_sampler[rel] = torch.utils.data.DistributedSampler(atomic_flattened['train'][rel],shuffle=shuffle)\n",
        "        train_dataloader[rel] = torch.utils.data.DataLoader(atomic_flattened['train'][rel],\n",
        "            batch_size=node_batch_size,sampler=train_sampler[rel],\n",
        "            collate_fn=collate_fn_for_flattened)\n",
        "        if is_main_process:\n",
        "            print(\"dev data loader\")\n",
        "            dev_dataloader[rel] = torch.utils.data.DataLoader(atomic_flattened['validation'][rel],\n",
        "                batch_size=node_batch_size,shuffle=shuffle_evaluation,\n",
        "                collate_fn=collate_fn_for_flattened)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "title": "prepare for training",
        "id": "0r-0kX-Xs8YC"
      },
      "source": [
        "    if is_main_process:\n",
        "        print(\"init sw to: \", save_path) \n",
        "        sw = SummaryWriter(save_path)\n",
        "        serialization_dir = save_path\n",
        "        tokenizer.save_pretrained(serialization_dir)\n",
        "        with open(os.path.join(serialization_dir,'exp_config.json'),'w') as f:\n",
        "            import json\n",
        "            json.dump(cfg,f,ensure_ascii=False,indent=4)\n",
        "    for wrapped_model in wrapped_models.values():\n",
        "        wrapped_model.to(device=device)\n",
        "\n",
        "    if ddp:\n",
        "        for rel in wrapped_models:\n",
        "            wrapped_models[rel] = torch.nn.parallel.DistributedDataParallel(wrapped_models[rel],device_ids=[local_rank])\n",
        "    # no_decay = ['bias', 'LayerNorm.weight']\n",
        "    # optimizer_grouped_parameters = [\n",
        "    #     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "    #     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    # ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "QuQfBexcs8YD"
      },
      "source": [
        "    for rel,wrapped_model in wrapped_models.items():\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\":[p for p in wrapped_model.parameters() if p.requires_grad]}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters,lr=learning_rate,eps=1e-8)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,warm_up_steps,iterations)\n",
        "        step = 0\n",
        "        done = True\n",
        "        best_dev_loss = 1e10\n",
        "        train_iter = iter(train_dataloader[rel])\n",
        "        if is_main_process:\n",
        "            pbar = tqdm(total=iterations,dynamic_ncols=True,desc=rel)\n",
        "        while step <= iterations:\n",
        "            try:\n",
        "                if is_main_process and (step % cycle == 0 and step > 0): #validation\n",
        "                    #print(\"start validating...\")\n",
        "                    with torch.no_grad():\n",
        "                        if ddp:\n",
        "                            wrapped_model.module.update_model_weight()\n",
        "                        else:\n",
        "                            wrapped_model.update_model_weight()\n",
        "                        if frozen:\n",
        "                            for p in model.parameters():\n",
        "                                p.requires_grad = False \n",
        "                        model.eval()\n",
        "                        pbar.set_description(f'validating...{rel}')\n",
        "                        dev_allset_micro_loss = 0.\n",
        "                        dev_token_loss = 0.\n",
        "                        dev_token_count = 0\n",
        "                        dev_sample_loss = 0. #avg on sample\n",
        "                        dev_sample_count = 0\n",
        "                        for batch in tqdm(dev_dataloader[rel],desc=f'validating ...',leave=False):\n",
        "                            if dev_sample_count>=validation_size:\n",
        "                                break\n",
        "                            batch = {k:v.to(device=device) for k,v in batch.items()}\n",
        "                            result = model(**batch)\n",
        "                            # logits = clip_logits(result['logits'])\n",
        "                            logits = result['logits']\n",
        "                            loss = torch.nn.functional.cross_entropy(\n",
        "                                logits.reshape(-1,logits.size(2)),\n",
        "                                batch['labels'].reshape(-1,),\n",
        "                                reduction='none'\n",
        "                            ).reshape(logits.size(0),-1)\n",
        "                            labels_mask = (batch['labels'] != -100) \n",
        "                            dev_token_loss += loss.sum().item()\n",
        "                            dev_token_count += labels_mask.sum().item()\n",
        "                            dev_sample_loss += (loss.sum(dim=-1)/labels_mask.sum(dim=-1)).sum().item()\n",
        "                            dev_sample_count += logits.size(0)\n",
        "                            del result\n",
        "                            del loss\n",
        "                            del labels_mask\n",
        "                        dev_micro_avg_loss = dev_token_loss/dev_token_count\n",
        "                        dev_macro_avg_loss = dev_sample_loss/dev_sample_count\n",
        "                        sw.add_scalar(f'dev/{rel}/micro_avg_loss',dev_micro_avg_loss,step)\n",
        "                        sw.add_scalar(f'dev/{rel}/macro_avg_loss',dev_macro_avg_loss,step)\n",
        "                        if dev_micro_avg_loss < best_dev_loss:\n",
        "                            best_dev_loss = dev_micro_avg_loss\n",
        "                            model.save_pretrained(serialization_dir)\n",
        "                        #ggg\n",
        "                        if step == iterations:\n",
        "                            generation_results = \\\n",
        "                            \"|Queries|Generation Results| Target |\\n\"\\\n",
        "                            \"|-|-|-|\\n\"\n",
        "                            for i,(query,responses) in enumerate(tqdm(atomic_query_responses['validation'][rel].items())):\n",
        "                            #for i,key in enumerate(tqdm(atomic_query_responses['validation'][rel])):\n",
        "                                if i==validation_num_generation:\n",
        "                                    break\n",
        "                                results = tokenizer.batch_decode(\n",
        "                                    model.generate(**tokenizer(query, return_tensors='pt').to(device=device),**generation_params),\n",
        "                                    skip_special_tokens=True\n",
        "                                )\n",
        "                                generation_results+=f\"|`{query}`|`{str(results)}`|`{str(responses)}`|\\n\"\n",
        "                            sw.add_text(f'dev/{rel}/generation_samples',generation_results,step)\n",
        "                # end validation\n",
        "                if step > 4000 and frozen and not done:\n",
        "                    print(\"unfreezing the model\")\n",
        "                    done = True\n",
        "                    for p in model.parameters():\n",
        "                        p.requires_grad = True # Unfreezing\n",
        "                if step > 4000 and not frozen and not done:\n",
        "                    print(\"freezing the model\")\n",
        "                    done = True\n",
        "                    for p in model.parameters():\n",
        "                        p.requires_grad = False # freezing\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                batch_loss = torch.tensor(0.)\n",
        "                for tiny_step in range(accumulation_tiny_steps):\n",
        "                    try:\n",
        "                        batch = next(train_iter)\n",
        "                    except StopIteration:\n",
        "                        train_iter = iter(train_dataloader[rel])\n",
        "                        batch = next(train_iter)\n",
        "                    batch = {k:v.to(device=device) for k,v in batch.items()}\n",
        "                    result = wrapped_model(**batch)\n",
        "                    loss = result['loss']/accumulation_tiny_steps\n",
        "                    # logits = clip_logits(result['logits'])\n",
        "                    # loss = torch.nn.functional.cross_entropy(\n",
        "                    #     logits.reshape(-1,logits.size(2)),\n",
        "                    #     batch['labels'].reshape(-1,)\n",
        "                    # )/accumulation_tiny_steps\n",
        "                    loss.backward()\n",
        "                    batch_loss += loss.item()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                step+=1\n",
        "                if ddp:\n",
        "                    # loss = loss.detach()\n",
        "                    losses = [torch.zeros_like(batch_loss) for i in range(world_size)]\n",
        "                    torch.distributed.all_gather(tensor_list=losses,tensor=batch_loss)\n",
        "                    batch_loss = torch.stack(losses).mean()\n",
        "                if is_main_process:\n",
        "                    pbar.set_description(f'training...{rel}')\n",
        "                    pbar.update()\n",
        "                    sw.add_scalar(f'train/{rel}/loss',batch_loss.item(),global_step=step)\n",
        "                del result\n",
        "                del loss\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"exiting while ...\")\n",
        "                break\n",
        "            # end train while\n",
        "        if is_main_process:\n",
        "            pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "15Gp0b24s8YE"
      },
      "source": [
        "\n",
        "    results = []\n",
        "    #device = 'cuda:0'\n",
        "    #model = model.to(device)\n",
        "    val_set = \"validation\"\n",
        "    scorer_model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
        "    #sss\n",
        "    df = data_df[val_set]\n",
        "    target = \"target_text\"\n",
        "    df[target] = df[target].astype(str)\n",
        "    df = df.groupby(['prefix','input_text'],as_index=False)[target].agg('<br />'.join)\n",
        "    print(\"Scoring...\")\n",
        "    if num_generations>0:\n",
        "        df = df.truncate(after=num_generations)\n",
        "\n",
        "    for rel in [\"xIntent\"]:\n",
        "        sum_score = 0 \n",
        "        total = num_generations\n",
        "        if num_generations == 0:\n",
        "            total = len(atomic_query_responses[val_set][rel].items())\n",
        "        pbar = tqdm(atomic_query_responses[val_set][rel].items(), total = total)\n",
        "        for idx,(query,responses) in enumerate(pbar):\n",
        "            if num_generations>0 and idx>= num_generations:\n",
        "                break\n",
        "            hyps = tokenizer.batch_decode(\n",
        "                            model.generate(**tokenizer(query, return_tensors='pt').to(device=device),**generation_params),\n",
        "                            skip_special_tokens=True\n",
        "                        )\n",
        "            query = re.sub(r'<.*?>','',query)\n",
        "            tails = [x[1] for x in responses]\n",
        "  \n",
        "            sents1 = tails\n",
        "            sents2 = hyps\n",
        "\n",
        "            #Compute embeddings\n",
        "            embeddings1 = scorer_model.encode(sents1, device=device, convert_to_tensor=True)\n",
        "            embeddings2 = scorer_model.encode(sents2, device=device, convert_to_tensor=True)\n",
        "\n",
        "            #Compute cosine-similarities for each sentence with each other sentence\n",
        "            cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "            #Find the pairs with the highest cosine similarity scores\n",
        "            pairs = []\n",
        "            rows = cosine_scores.shape[0]\n",
        "            cols = cosine_scores.shape[1]\n",
        "            for i in range(rows):\n",
        "                for j in range(cols):\n",
        "                    pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "                #print({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "        #Sort scores in decreasing order\n",
        "            pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "            top = pairs[0]\n",
        "            pred_text = str(sents2[top[\"index\"][1]])\n",
        "            closest = str(sents1[top[\"index\"][0]])\n",
        "            cond = (df['prefix'] == rel) & (df['input_text'] == query)\n",
        "            df.loc[cond, \"top\"] = closest\n",
        "            df.loc[cond, \"pred_text1\"] = pred_text\n",
        "            df.loc[cond, \"pred1_score\"] = \"{:.4f}\".format(top[\"score\"])\n",
        "            cur_score = top[\"score\"]\n",
        "            sum_score += cur_score\n",
        "            mean_score = \"{:.4f}\".format(sum_score / idx)\n",
        "            #tqdm.write(f\"Mean score:{mean_score}\")\n",
        "            print(query, \"\\n\" , pred_text, \"\\n\", closest)\n",
        "            pbar.set_description(f\"Mean score:{mean_score} cur score {cur_score:.2f}\")\n",
        "            pbar.update(1)\n",
        "\n",
        "            results.append({\n",
        "                \"head\":query,\n",
        "                \"gens\":hyps,\n",
        "                \"tails\":tails,\n",
        "            })\n",
        "        # %%%%%%%%%%%%%%%%%%\n",
        "    print(\"Results:\", df[\"pred1_score\"].mean())\n",
        "    pbar.close()\n",
        "    with open(os.path.join(save_path,f\"{val_set}_gen.json\"),'w') as f:\n",
        "        json.dump(results,f,ensure_ascii=False,indent=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "hL4gS065s8YE"
      },
      "source": [
        "    out = save_path + \"/scored_\" + model_name  + \".tsv\" \n",
        "    print(out)\n",
        "    print(len(df))\n",
        "    df.to_csv(out, sep=\"\\t\", index=False)\n",
        "    with open(\"/home/pouramini/dflist\", \"w\") as dflist:\n",
        "        print(model_name,\"=\",out, file=dflist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zwYTqMLs8YF"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}